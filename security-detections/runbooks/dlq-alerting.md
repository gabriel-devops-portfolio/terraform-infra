ðŸš¨ Runbook: SOC Alerting DLQ Incident

ðŸ“Œ Purpose

This runbook defines response procedures when the SOC alerting Dead Letter Queue (DLQ)
contains messages, indicating a failure in the alert delivery pipeline.

DLQ alerts represent an operational monitoring failure rather than a direct security
incident. However, they are treated as high priority because failed delivery may prevent
security alerts from reaching responders.

â¸»

## Severity
High (Operational)

## Estimated Investigation Time
**30-60 minutes**

## Compliance Scope
- **SOC 2:** CC7.2 (System Monitoring)
- **PCI-DSS:** 10.6 (Log Review)
- **HIPAA:** 164.312(b) (Audit Controls)
- **GDPR:** Article 32 (Security Monitoring)

â¸»

ðŸ” Trigger Condition

â€¢ CloudWatch alarm: soc-dlq-messages-present
â€¢ Condition: One or more messages present in the DLQ

â¸»

ðŸŽ¯ Impact

â€¢ Security alerts may not be delivered to responders
â€¢ SOC visibility may be degraded
â€¢ Potential missed or delayed incident response

â¸»

## OpenSearch Query Example

**DLQ Message Inspection (AWS CLI):**
```bash
# List DLQ messages
aws sqs get-queue-attributes \
  --queue-url https://sqs.us-east-1.amazonaws.com/111111222222/soc-security-alerts-dlq \
  --attribute-names ApproximateNumberOfMessages

# Receive and inspect messages (without deleting)
aws sqs receive-message \
  --queue-url https://sqs.us-east-1.amazonaws.com/111111222222/soc-security-alerts-dlq \
  --max-number-of-messages 10 \
  --visibility-timeout 0

# Check CloudWatch metrics
aws cloudwatch get-metric-statistics \
  --namespace AWS/SQS \
  --metric-name ApproximateNumberOfMessagesVisible \
  --dimensions Name=QueueName,Value=soc-security-alerts-dlq \
  --start-time 2026-01-12T00:00:00Z \
  --end-time 2026-01-12T23:59:59Z \
  --period 300 \
  --statistics Sum
```

**CloudWatch Log Insights (SNS Delivery Failures):**
```
fields @timestamp, @message
| filter @message like /delivery failure/ or @message like /AccessDenied/
| sort @timestamp desc
| limit 50
```

â¸»

ðŸ§  Investigation Steps

1. Confirm the DLQ alarm state in CloudWatch
2. Identify the affected SQS DLQ queue
3. Check message count and oldest message age
4. Inspect sample messages to identify the failure source
5. Determine which component failed to deliver alerts:
   â€¢ OpenSearch notification destination
   â€¢ SNS topic or subscription
   â€¢ IAM permissions associated with alert delivery
6. Review recent configuration or IAM changes
7. Check service quotas, throttling, or regional service issues

â¸»

ðŸ› ï¸ Containment Actions

â€¢ Manually notify SOC stakeholders if critical alerts may be blocked
â€¢ Temporarily pause affected OpenSearch monitors if misfiring
â€¢ Ensure no high-severity security alerts are silently dropped

â¸»

ðŸ”„ Remediation Steps

â€¢ Fix IAM permission or configuration issues
â€¢ Validate OpenSearch notification destinations
â€¢ Confirm SNS topic and subscription health
â€¢ Reprocess or manually review DLQ messages if required
â€¢ Clear DLQ messages only after resolution is confirmed

â¸»

âœ… Validation

â€¢ Confirm DLQ message count returns to zero
â€¢ Trigger a test alert from OpenSearch
â€¢ Verify successful delivery to SNS and email recipients
â€¢ Confirm CloudWatch alarm returns to OK state

â¸»

## Common Root Causes

- **IAM Permissions:** OpenSearchSNSRole missing `sns:Publish` permission
- **SNS Subscription:** Email not confirmed, subscription deleted
- **Service Quotas:** SNS message rate limit exceeded
- **Network Issues:** VPC endpoint or security group blocking SNS
- **Malformed Payloads:** OpenSearch monitor JSON syntax errors

**Resolution Time:** 15-30 minutes for IAM/config issues

â¸»

## Escalation Criteria

**Escalate to Platform Team if:**
- DLQ messages exceed 50
- Oldest message age >2 hours
- Multiple monitors affected simultaneously
- Cannot identify root cause within 30 minutes

**Notification:** Slack #platform-engineering, PagerDuty

â¸»

ðŸ“˜ Lessons Learned

â€¢ Document root cause and remediation steps
â€¢ Identify whether retries, thresholds, or permissions require tuning
â€¢ Evaluate whether alert delivery resilience needs improvement

â¸»

ðŸ”— Related Components

â€¢ **CloudWatch Alarm:** DLQ monitoring (`soc-dlq-messages-present`)
â€¢ **SQS DLQ:** `soc-security-alerts-dlq`
â€¢ **SNS Topics:** `soc-alerts-critical`, `soc-alerts-high`, `soc-alerts-medium`
â€¢ **IAM Role:** `OpenSearchSNSRole`
â€¢ **OpenSearch:** Notification destinations and monitors
â€¢ **AWS Security Lake:** Telemetry source

â¸»

## Related Runbooks

- [GuardDuty Detection](./guardduty.md) - Critical alerts that may be affected
- [Root Account Usage](./root-account.md) - Critical alerts that must be delivered
- [VPC Scanning](./vpc-scanning.md) - Medium alerts affected by delivery failures

â¸»

ðŸ§  SOC Note

Detection without delivery is failure.

A healthy SOC pipeline ensures that every critical alert
reaches a human responder without delay.
